\input{preamble.tex}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{appendix}
\usepackage{csquotes}
\usepackage{titlesec}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{floatrow}
\usepackage{float}
\lstset{
    frame = single,
    breaklines=true,
    basicstyle=\ttfamily}
\usepackage{tikz, forest}
\usepackage{natbib}

\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}


\begin{document}

\header{Assignment 4}{Zhang Rongqi}{A0276566M}

\section{Implementation}

\begin{itemize}
    \item \texttt{get\_plane\_sweep\_homographies()}: In this function, a list of homograph matrices are computed. Firstly I recover the $\textbf{R}$ and $\textbf{C}$ from \texttt{relative\_pose} matrix, specifically, $\textbf{M}_{rel} = [\textbf{R}^\intercal|\textbf{R}^\intercal \textbf{C}]$ (don't know why it is not $[\textbf{R}^\intercal|-\textbf{R}^\intercal \textbf{C}]$). In this case, $\mathbf{K}=\mathbf{K}_{ref}$ and $\textbf{n}=(0,0,1)^\intercal$. Then \texttt{homographies} is calculated by $\mathbf{K}(\mathbf{R}^\intercal + \frac{\mathbf{R}^\intercal \mathbf{C} \mathbf{n}^\intercal}{d})\mathbf{K}^{-1}$
    
    \item \texttt{compute\_plane\_sweep\_volume()}: In this function, I iterate each $(D, H, W)$ in the reference coordinate. Firstly, I used \texttt{concat\_extrinsic\_matrix(ref\_pose,\\
    invert\_extrinsic(images[i].pose\_mat))} to calculate \texttt{relative\_pose}, which can convert each image to reference coordinate. Then this \texttt{relative\_pose} is used in \texttt{get\_plane\_sweep\_homographies()}. The \texttt{homographies} is used to call \\
    \texttt{cv2.warpPerspective()} to get \texttt{warped\_images} and \texttt{warped\_maskes}. \\ \texttt{warped\_maskes} are used to update \texttt{accum\_count}. I tried two approaches to calculate the variance. One way is purely "variance": Two iterations are used to calculate the mean and variance for the same pixel and depth of \texttt{warped\_images}. But this result is quite noisy. Then I tried to use L1 loss as variance, and set \texttt{kernel\_size=(3,3)}, this method produced a better result. This result is so good that makes the effect of my \texttt{post\_process()} is not much noticeable.

    \item \texttt{compute\_depths()}: In this function, I simply picked the depth which corresponds to the minimal variance.
    
    \item \texttt{post\_process()}: Firstly, I utilized \texttt{scipy.ndimage.median\_filter()} to smooth \texttt{ps\_volume}. I set the window size to $(3,5,5)$. Then, I use sigma detection for each depth to filter out unreliable positions. Specifically, I iterate over all pixels to verify if \texttt{accum\_count[idx[h, w], h, w] < accum\_mean[idx[h, w]] - accum\_var[idx[h, w]]} or \texttt{smooth\_ps\_volume[idx[h, w], h, w] > ps\_mean[idx[h, w]] + ps\_var[idx[h, w]]}. If not, I set the mask as false.


    \item \texttt{unproject\_depth\_map()}: Firstly I checked the parameter \texttt{mask} exists or not. If not \texttt{mask} is initialized as all one. Then I used \texttt{np.meshgrid()} to get all the indices, and used the \texttt{mask} to filter the unreliable indices. In this case, the projective matrix is simply set as $\textbf{K}[\textbf{I}|\textbf{0}]$. So 3D points can be calculated by $\texttt{points3d} = \textbf{K}^{-1}(x, y, 1)^\intercal$, where the depth of \texttt{points3d} is corresponding reciprocal of \texttt{inv\_depth\_image}  $x$ and $y$ are iterated from the image size, and corresponding \texttt{pointsrgb} is set as \texttt{image[y, x]}. 

\end{itemize}

\end{document}


